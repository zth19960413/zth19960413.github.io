[{"title":"Hadoop伪分布式集群的搭建","url":"/2019/05/15/Hadoop伪分布式集群的搭建/","content":"我在后面学习Hive的时候，在最开始我的Hadoop集群搭建的有问题，所以我决定重头搭建，然后做出笔记。写了一些自己遇到的坑，这里来把这些坑给填上。\n\nHadoop基本安装配置主要包括以下5个步骤。\n（1）创建Hadoop用户\n（2）安装Java\n（3）设置SSH登录权限\n（4）单机安装配置\n（5）伪分布式安装配置\n我使用的操作系统是Ubuntu14.4，Hadoop版本为2.7.3\n\n###第一步，先安装一个Linux系统，创建Hadoop用户\n我用的是VMware虚拟机，我大学生涯就用了这一个虚拟机，感觉短小精悍功能强大。\n我选择的是Ubuntu系统，因为我感觉Ubuntu的系统用起来比较舒服，并且遇到问题在网络上写Ubuntu的解决方案也比较多，推荐使用Ubuntu。\n![](http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515175725.png)\n这里我就完成了第一步的操作\n在创建虚拟机的过程中我就建立了一个Hadoop用户，为了规范，在使用Linux系统对于用户比较敏感，因为Linux是多用户的。尽量养成一种好的习惯，名字为Hadoop用户那么这个用户就是关于Hadoop的。\n\n###第二步，安装Java\n我以为是新装的Ubuntu不能连接FileZilla，需要下载ssh\n\n安装ssh\n```\nsudo apt-get install openssh-server\n```\n\n安装完后查看ssh server是否启动\n```\nsudo /etc/init.d/ssh status\n```\n\n如果没有启动，使用一下命令启动：\n```\nsudo /etc/init.d/ssh start\n```\n\n根据ifconfig查看IP，用FileZilla连接，使用非常简单一看就会。\n网上下载一个JDK-linux版本的\n\n这里遇到了一个坑，一定要注意和自己Linux位数相同的看好自己的是32位还是64位，我下的12.0.1版本的JDK\n```\nsudo uname --m\n```\n\n之后就是JDK的配置问题，网络有有好多的配置方法可以参考\n\n####特别注意\n在配置环境变量的时候\n```\nsudo gedit ~/.bashrc    #这里配置是给当前用户配置的环境变量，如果用的prefile配置则不考虑用户的问题，特别注意\n```\n\n###第三步，SSH登录权限设置，设置无密码登录\n\n```\n$ sudo apt-get install openssh-server   #安装SSH server\n$ ssh localhost                         #登陆SSH，第一次登陆输入yes\n$ exit                                  #退出登录的ssh localhost\n$ cd ~/.ssh/                            #如果没法进入该目录，执行一次ssh localhost\n$ ssh-keygen -t rsa　　\n```\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515203918.png\"  />\n\n\n其中，第一次回车是让KEY存于默认位置，以方便后续的命令输入。第二次和第三次是确定passphrase，相关性不大。两次回车输入完毕以后，如果出现类似于下图所示的输出，即成功：\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515203954.png\"  />\n\n\n之后再输入\n```\n$ cat ./id_rsa.pub >> ./authorized_keys #加入授权\n$ ssh localhost                         #此时已不需密码即可登录localhost\n```\n###第四步，单机安装Hadoop\n\n我安装的Hadoop版本为2.7.3，下载地址为http://hadoop.apache.org/releases.html#Download，\n在目录中选择hadoop-2.7.3.tar.gz进行下载就行了\n\n我是放在了/usr/local/haddoop文件夹下，注意这里一定要改这个文件夹的权限\n```\nsudo chown -R hadoop ./hadoop                        #修改文件权限\n```\n\n然后给hadoop配置环境变量，在当前用户下配置\n```\ngedit ~/.bashrc\n```\n\n添加如下代码：\n```\nexport HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.3  #这里的路径为你安装hadoop的文件夹\nexport CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n####注意：一定要看好自己装的文件夹，别把Hadoop的上级文件放上去。就是在这个目录下一级是如下图所示：\n\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515205305.png\"  />\n\n\n在使用source ~/.bashrc命令出现如下图，证明你迈出了第一步，安装好了Hadoop但是没有集群的搭建，不要放弃，也不要觉得配置完了\n接下来才是更大的难题，更容易出错的地方\nhttp:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515205548.png\n\n###第五步，Hadoop伪分布式安装\n\nHadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。首先将jdk1.7的路径添（export JAVA_HOME=/usr/lib/jvm/java ）加到hadoop-env.sh文件 \n\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515211148.png\"  />\n\n\n接下来修改core-site.xml文件:\n```\n<configuration>\n <name>hadoop.tmp.dir</name>\n             <value>file:/usr/local/hadoop/hadoop-2.7.3/tmp</value>\n             <description>Abase for other temporary directories.</description>\n        </property>\n        <property>\n             <name>fs.defaultFS</name>\n             <value>hdfs://localhost:9000</value>\n        </property>\n</configuration>\n```\n####还要特别注意这里面的路径都要写你安装hadoop的路径，千万别错了。后面出问题找很麻烦\n这里要注意要创建一个tmp的文件夹，防止以后有问题出现\n\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515211448.png\"  />\n\n\n然后配置hdfs-site.xml文件\n```\n<configuration>\n<property>\n             <name>dfs.replication</name>\n             <value>1</value>\n        </property>\n        <property>\n             <name>dfs.namenode.name.dir</name>\n             <value>file:/usr/local/hadoop/hadoop-2.7.3/tmp/dfs/name</value>\n        </property>\n        <property>\n             <name>dfs.datanode.data.dir</name>\n             <value>file:/usr/local/hadoop/hadoop-2.7.3/tmp/dfs/data</value>\n        </property>\n</configuration>\n```\n###注意：这里的路径也是安装路径\n\nHadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（可参考官方教程），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。\n\n配置完成后，执行 NameNode 的格式化\n```\n./bin/hdfs namenode -format\n```\n\n如果出现bash: ./bin/hdfs: No such file or directory\n\n则可以去你Hadoop安装目录下的bin目录运行，是一样的\n```\nhdfs namenode -format\n```\n最后出现如下图，那么证明成功了\n\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515212307.png\"  />\n\n\n启动namenode和datanode进程，并查看启动结果\n```\n./sbin/start-dfs.sh\njps\n```\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515223447.png\"  />\n\n\n然后停止\n```\n./sbin/stop-dfs.sh\njps\n```\n\n看看全启动是否成功，运行下面命令\n```\nstart-all.sh\njps\n```\n####如果出现下面的图就说明你安装成功了 恭喜你！！！\n\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515223929.png\"  />\n\n\n成功启动后，可以访问 Web 界面 http://localhost:50070 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。\n\n<img src=\"http:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515224126.png\"  />\n\n\n【更多的细节可以参照https://www.cnblogs.com/87hbteo/p/7606012.html 感谢大佬】\n\t【还有林子雨编写的《大数据技术 原理与应用》】\n\n\n\n\n\n\n\n\t\n\t\n","tags":["Hadoop","Ubuntu","分布式"]}]