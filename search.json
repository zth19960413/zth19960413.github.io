[{"title":"基于Hadoop的数据仓库Hive安装","url":"/2019/05/16/基于Hadoop的数据仓库Hive安装/","content":"\n前面的博客，我们已经安装好了Hadoop，对于Hadoop的也有了比较深的了解。下面是我对数据仓库Hive的安装\n\n## 一、安装Hive\n\n**Hive的下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz**\n\n由于我下载的这个版本，所以我给的链接是这个版本的\n\n```\nsudo tar -zxvf ./apache-hive-1.2.1-bin.tar.gz -C /usr/local   # 解压到/usr/local中\ncd /usr/local/\nsudo mv apache-hive-1.2.1-bin hive       # 将文件夹名改为hive\nsudo chown -R hadoop:hadoop hive            # 修改文件权限\n```\n\n>注意，上面的hadoop：hadoop是用户组和用户名\n\n### 配置环境变量\n\n为了方便使用，我们把hive命令加入到环境变量中去，\n\n请使用gedit编辑器打开/etc/profile文件，命令如下：\n\n```\nsudo gedit /etc/profile\n```\n\n在文件最后一行加入下面内容：\n```\nexport HIVE_HOME=/usr/local/hive \nexport PATH=$PATH:$HIVE_HOME/bin \n```\n\n这里还需要HADOOP_HOME因为在上一篇或者以前你已经配好了，那就不用再写了\n\n在打开一个终端运行下面命令\n```\nsu \nsource /etc/profile\n```\n\n### 修改/usr/local/hive/conf下的hive-site.xml\n\n执行如下命令：\n\n```\ncd /usr/local/hive/conf\nmv hive-default.xml.template hive-default.xml\n```\n\n上面命令是将hive-default.xml.template重命名为hive-default.xml；\n\n然后，使用gedit编辑器新建一个配置文件hive-site.xml，命令如下：\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n  <property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>\n    <description>JDBC connect string for a JDBC metastore</description>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n    <description>Driver class name for a JDBC metastore</description>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>hive</value>\n    <description>username to use against metastore database</description>\n  </property>\n  <property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>hive</value>\n    <description>password to use against metastore database</description>\n  </property>\n</configuration>\n```\n>注意：这里一定不要写错了，如果写错了一个字，后面会出很大的问题，查找起来特别麻烦，就像C语言指针错了一样。很烦\n\n\n## 二、安装并配置mysql\n\n**这里我认为是最麻烦的，因为容易安装失败，安装失败后，一定要卸载干净再下载**\n\n**可能大家在Windows安装mysql有心态炸的时候吧，还记得那四个对号。我记得我当初心态炸了**\n\n这里我们采用MySQL数据库保存Hive的元数据，而不是采用Hive自带的derby来存储元数据。\n\nubuntu上安装mysql非常简单只需要几条命令就可以完成。\n\n```\nsudo apt-get install mysql-server\nsudo apt-get install mysql-client\nsudo apt-get install libmysqlclient-dev\n```\n\n安装过程中会提示设置密码什么的，注意设置了不要忘了，安装完成之后可以使用如下命令来检查是否安装成功：\n\n```\nsudo netstat -tap | grep mysql\n```\n\n通过上述命令检查之后，如果看到有mysql 的socket处于 listen 状态则表示安装成功。\n \n登陆mysql数据库可以通过如下命令：\n\n```\nmysql -u root -p \n```\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190516183158.png\"/>\n\n如果出现如上突脸看到mysql 的socket处于 listen 状态则表示安装成功。\n\n登陆mysql数据库可以通过如下命令：\n```\nmysql -u root -p \n```\n-u 表示选择登陆的用户名， -p 表示登陆的用户密码，上面命令输入之后会提示输入密码，此时输入密码就可以登录到mysql。\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190516183347.png\" />\n\n最后出现这个证明你完全没有问题了\n\n### 下面配置mysql的jdbc\n\n**mysql jdbc的包下载地址：https://dev.mysql.com/downloads/connector/j/\n\n或者通过别的渠道，我这里下载的5.1.4版本的\n\n下载后运行如下命令\n```\ntar -zxvf mysql-connector-java-5.1.40.tar.gz   #解压\ncp mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar  /usr/local/hive/lib #将mysql-connector-java-5.1.40-bin.jar拷贝到/usr/local/hive/lib目录下\n```\n\n### 启动并登陆mysql shell\n\n```\nservice mysql start #启动mysql服务\nmysql -u root -p  #登陆shell界面\n```\n\n### 新建hive数据库\n```\nmysql> create database hive;    #这个hive数据库与hive-site.xml中localhost:3306/hive的hive对应，用来保存hive元数据\n```\n>注意：在运行mysql和以后运行hive命令，一定不要忘了后面加；号，要不命令是不会生效的，系统默认还没有完成命令的编写。\n\n### 配置mysql允许hive接入：\n\n```\nmysql> grant all on *.* to hive@localhost identified by 'hive';   #将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码\nmysql> flush privileges;  #刷新mysql系统权限关系表\n```\n\n## 启动hive\n\n启动hive之前，先启动hadoop集群\n\n```\nstart-all.sh #启动hadoop\n```\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190516184518.png\"  />\n\n如果出现上面图出现6个，那说明启动成功了，如果不成功，看那个没出来去网上查找解决方案\n\n然后启动hive\n```\nhive  #启动hive\n```\n\n>注意，我们这里已经配置了PATH，所以，不要把start-all.sh和hive命令的路径加上。如果没有配置PATH，请加上路径才能运行命令，比如，本教程Hadoop安装目录是“/usr/local/hadoop”，Hive的安装目录是“/usr/local/hive”，因此，启动hadoop和hive，也可以使用下面带路径的方式：\n\n```\ncd /usr/local/hadoop\n./sbin/start-all.sh\ncd /usr/local/hive\n./bin/hive\n```\n\n如果出现如下错误\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190516200558.png\"  />\n\n则需要进入到hadoop安装目录下的share/hadoop/yarn/lib下删除jline-0.9.94.jar文件，再启动hive即可（因为高版本的Hadoop对Hive有捆绑）。\n\n如果出现，这里我一直在处理问题，我忘了截图了，就是出现(jdbc-type=\"\", sql-type=\"\") 这种的错误，是因为jdk的版本过高，造成编译过程中出现了问题\n\n```\n Datastore.Schema (Log4JLogger.java:error(125)) - Failed initialising database.\nThe java type java.lang.Integer (jdbc-type=\"\", sql-type=\"\") cant be mapped for this datastore. No mapping is available.\norg.datanucleus.exceptions.NucleusException: The java type java.lang.Integer (jdbc-type=\"\", sql-type=\"\") cant be mapped for this datastore. No mapping is available.\n\n```\n**特别注意！！！：这里由于我使用的12版本的JKD出现的问题，查找了好久，最后降了版本使用了1.8的就好使了，以后不管干什么不要用高版本的，记住高版本兼容低版本，而低版本用不了高版本！一定要记住不管干什么，要不然查找问题起来特别的麻烦，我也修改了上个博客。**\n\n最后在运行hive\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190516200943.png\"  />\n\n那么你成功了！恭喜你！\n\n可以在里面输入SQL语句，如果要退出Hive交互式执行环境，可以输入如下命令：\n\n```\nhive>exit;\n```\n\n**这里还要注意一点：**\n\n**启动完hive后，启动另外的中段运行jps，看看ResourceManager有没有死掉，如果死掉了。对于后面Hive的使用会出问题。**\n\n**简单介绍下ResourceManager**\n\n**ResourceManager (RM)是仲裁所有可用集群资源的主程序，从而帮助管理在纱线系统上运行的分布式应用程序。它与每个节点的节点管理器(NMs)和每个应用程序的应用程序管理器(AMs)一起工作。**\n\n## 推荐\n\n如果你有英语的一定基础，或者你可以看懂英文的文档推荐一个网站https://zh.hortonworks.com/blog/apache-hadoop-yarn-resourcemanager/\n\n这个网站介绍了ResourceManager，你也可以查找相关的文档，英文的文档我认为更加的细致。\n\n如果你Java的变成能力特别好，推荐你看Hadoop的底层源码。\n\n\n\n\n## 参考：\n\n【林子雨编写的《大数据 基础编程、实验和案例教程》】\n\n\n\n\n\n\n\n\t\n\t\n","tags":["Hadoop","Hive","数据仓库"]},{"title":"Hadoop伪分布式集群的搭建","url":"/2019/05/15/Hadoop伪分布式集群的搭建/","content":"我在后面学习Hive的时候，在最开始我的Hadoop集群搭建的有问题，所以我决定重头搭建，然后做出笔记。写了一些自己遇到的坑，这里来把这些坑给填上。\n\nHadoop基本安装配置主要包括以下5个步骤。\n（1）创建Hadoop用户\n（2）安装Java\n（3）设置SSH登录权限\n（4）单机安装配置\n（5）伪分布式安装配置\n我使用的操作系统是Ubuntu14.4，Hadoop版本为2.7.3\n\n## 第一步，先安装一个Linux系统，创建Hadoop用户\n我用的是VMware虚拟机，我大学生涯就用了这一个虚拟机，感觉短小精悍功能强大。\n我选择的是Ubuntu系统，因为我感觉Ubuntu的系统用起来比较舒服，并且遇到问题在网络上写Ubuntu的解决方案也比较多，推荐使用Ubuntu。\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515175725.png\"  />\n\n这里我就完成了第一步的操作\n在创建虚拟机的过程中我就建立了一个Hadoop用户，为了规范，在使用Linux系统对于用户比较敏感，因为Linux是多用户的。尽量养成一种好的习惯，名字为Hadoop用户那么这个用户就是关于Hadoop的。\n\n## 第二步，安装Java\n我以为是新装的Ubuntu不能连接FileZilla，需要下载ssh\n\n安装ssh\n```\nsudo apt-get install openssh-server\n```\n\n安装完后查看ssh server是否启动\n```\nsudo /etc/init.d/ssh status\n```\n\n如果没有启动，使用一下命令启动：\n```\nsudo /etc/init.d/ssh start\n```\n\n根据ifconfig查看IP，用FileZilla连接，使用非常简单一看就会。\n网上下载一个JDK-linux版本的\n\n这里遇到了一个坑，一定要注意和自己Linux位数相同的看好自己的是32位还是64位，我下的12.0.1版本的JDK\n\n>这里一定不要下高版本的JDK，我在后面的Hive数据仓库安装的时候出现了问题，最后我改成改了1.8版本的。\n\n```\nsudo uname --m\n```\n\n之后就是JDK的配置问题，网络有有好多的配置方法可以参考\n\n>特别注意：在配置环境变量的时候\nsudo gedit ~/.bashrc #这里配置是给当前用户配置的环境变量，如果用的prefile配置则不考虑用户的问题，特别注意\n\n## 第三步，SSH登录权限设置，设置无密码登录\n\n```\n$ sudo apt-get install openssh-server   #安装SSH server\n$ ssh localhost                         #登陆SSH，第一次登陆输入yes\n$ exit                                  #退出登录的ssh localhost\n$ cd ~/.ssh/                            #如果没法进入该目录，执行一次ssh localhost\n$ ssh-keygen -t rsa　　\n```\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515203918.png\"/>\n\n\n其中，第一次回车是让KEY存于默认位置，以方便后续的命令输入。第二次和第三次是确定passphrase，相关性不大。两次回车输入完毕以后，如果出现类似于下图所示的输出，即成功：\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515203954.png\"  />\n\n\n之后再输入\n```\n$ cat ./id_rsa.pub >> ./authorized_keys #加入授权\n$ ssh localhost                         #此时已不需密码即可登录localhost\n```\n## 第四步，单机安装Hadoop\n\n**我安装的Hadoop版本为2.7.3，下载地址为http://hadoop.apache.org/releases.html#Download**\n\n在目录中选择hadoop-2.7.3.tar.gz进行下载就行了\n\n我是放在了/usr/local/haddoop文件夹下，注意这里一定要改这个文件夹的权限\n```\nsudo chown -R hadoop ./hadoop                        #修改文件权限\n```\n\n然后给hadoop配置环境变量，在当前用户下配置\n```\ngedit ~/.bashrc\n```\n\n添加如下代码：\n```\nexport HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.3  #这里的路径为你安装hadoop的文件夹\nexport CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n```\n>注意：一定要看好自己装的文件夹，别把Hadoop的上级文件放上去。就是在这个目录下一级是如下图所示：\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515205305.png\"  />\n\n\n在使用source ~/.bashrc命令出现如下图，证明你迈出了第一步，安装好了Hadoop但是没有集群的搭建，不要放弃，也不要觉得配置完了\n接下来才是更大的难题，更容易出错的地方\nhttp:// prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515205548.png\n\n## 第五步，Hadoop伪分布式安装\n\nHadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。首先将jdk1.7的路径添（export JAVA_HOME=/usr/lib/jvm/java ）加到hadoop-env.sh文件 \n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515211148.png\"  />\n\n配置hadoop-env.sh文件，在hadoop安装目录下的/etc/hadoop/\n```\ngedit hadoop-env.sh\n```\n\n修改第27行\n\n```\nexport JAVA_HOME=你的JDK路径\n```\n\n接下来修改core-site.xml文件:\n```\n<configuration>\n <name>hadoop.tmp.dir</name>\n             <value>file:/usr/local/hadoop/hadoop-2.7.3/tmp</value>\n             <description>Abase for other temporary directories.</description>\n        </property>\n        <property>\n             <name>fs.defaultFS</name>\n             <value>hdfs://localhost:9000</value>\n        </property>\n</configuration>\n```\n>还要特别注意这里面的路径都要写你安装hadoop的路径，千万别错了。后面出问题找很麻烦\n>这里要注意要创建一个tmp的文件夹，防止以后有问题出现\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515211448.png\"  />\n\n\n然后配置hdfs-site.xml文件\n```\n<configuration>\n<property>\n             <name>dfs.replication</name>\n             <value>1</value>\n        </property>\n        <property>\n             <name>dfs.namenode.name.dir</name>\n             <value>file:/usr/local/hadoop/hadoop-2.7.3/tmp/dfs/name</value>\n        </property>\n        <property>\n             <name>dfs.datanode.data.dir</name>\n             <value>file:/usr/local/hadoop/hadoop-2.7.3/tmp/dfs/data</value>\n        </property>\n</configuration>\n```\n\n>注意：这里的路径也是安装路径\n\nHadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（可参考官方教程），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。\n\n配置完成后，执行 NameNode 的格式化\n```\n./bin/hdfs namenode -format\n```\n\n如果出现bash: ./bin/hdfs: No such file or directory\n\n则可以去你Hadoop安装目录下的bin目录运行，是一样的\n\n```\nhdfs namenode -format\n```\n\n最后出现如下图，那么证明成功了\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515212307.png\"  />\n\n\n启动namenode和datanode进程，并查看启动结果\n\n```\n./sbin/start-dfs.sh\njps\n```\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515223447.png\"  />\n\n\n然后停止\n\n```\n./sbin/stop-dfs.sh\njps\n```\n\n看看全启动是否成功，运行下面命令\n\n```\nstart-all.sh\njps\n```\n**如果出现下面的图就说明你安装成功了 恭喜你！！！**\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515223929.png\"  />\n\n\n成功启动后，可以访问 Web 界面 http://localhost:50070 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。\n\n<img src=\"http://prj1hj366.bkt.clouddn.com/qiniu_picGO/20190515224126.png\"  />\n\n## 参考：\n\n【更多的细节可以参照https://www.cnblogs.com/87hbteo/p/7606012.html 感谢大佬】\n\n【还有林子雨编写的《大数据技术 原理与应用》】\n\n\n\n\n\n\n\n\t\n\t\n","tags":["Hadoop","Ubuntu","分布式"]}]